{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9127c56b",
   "metadata": {},
   "source": [
    "### Task 8.1\n",
    "* Name:LI WAN\n",
    "* Student Number:223718804\n",
    "* E-mail:s223718804@deakin.edu.au\n",
    "* Course:SIT720\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bd2c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>-0.005957</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2.930406</td>\n",
       "      <td>9.487627</td>\n",
       "      <td>2.376523</td>\n",
       "      <td>6.187797</td>\n",
       "      <td>3.343416</td>\n",
       "      <td>-0.658054</td>\n",
       "      <td>-1.449106</td>\n",
       "      <td>-1.236256</td>\n",
       "      <td>0.601709</td>\n",
       "      <td>0.779642</td>\n",
       "      <td>0.813512</td>\n",
       "      <td>0.608385</td>\n",
       "      <td>0.023892</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3.392299</td>\n",
       "      <td>1.274827</td>\n",
       "      <td>2.954947</td>\n",
       "      <td>6.894759</td>\n",
       "      <td>4.349512</td>\n",
       "      <td>-1.663661</td>\n",
       "      <td>-0.952437</td>\n",
       "      <td>-1.733414</td>\n",
       "      <td>0.502079</td>\n",
       "      <td>0.567242</td>\n",
       "      <td>0.285880</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2.364034</td>\n",
       "      <td>2.842030</td>\n",
       "      <td>8.776391</td>\n",
       "      <td>1.008906</td>\n",
       "      <td>4.299976</td>\n",
       "      <td>-1.380719</td>\n",
       "      <td>-0.943884</td>\n",
       "      <td>-1.975373</td>\n",
       "      <td>0.487838</td>\n",
       "      <td>0.986505</td>\n",
       "      <td>0.149286</td>\n",
       "      <td>0.145984</td>\n",
       "      <td>-0.031810</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9.631511</td>\n",
       "      <td>3.994398</td>\n",
       "      <td>2.757071</td>\n",
       "      <td>7.821347</td>\n",
       "      <td>2.514755</td>\n",
       "      <td>-0.966330</td>\n",
       "      <td>-0.649915</td>\n",
       "      <td>-0.898510</td>\n",
       "      <td>0.365246</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.889118</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>6.530527</td>\n",
       "      <td>6.781790</td>\n",
       "      <td>4.349695</td>\n",
       "      <td>8.673138</td>\n",
       "      <td>3.492807</td>\n",
       "      <td>-1.390285</td>\n",
       "      <td>-1.532193</td>\n",
       "      <td>-0.570329</td>\n",
       "      <td>0.073056</td>\n",
       "      <td>0.505441</td>\n",
       "      <td>0.378761</td>\n",
       "      <td>0.942631</td>\n",
       "      <td>0.045263</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0     2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1     9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2     8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3     0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4     3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  2.930406  9.487627  2.376523  6.187797  3.343416 -0.658054 -1.449106   \n",
       "9996  3.392299  1.274827  2.954947  6.894759  4.349512 -1.663661 -0.952437   \n",
       "9997  2.364034  2.842030  8.776391  1.008906  4.299976 -1.380719 -0.943884   \n",
       "9998  9.631511  3.994398  2.757071  7.821347  2.514755 -0.966330 -0.649915   \n",
       "9999  6.530527  6.781790  4.349695  8.673138  3.492807 -1.390285 -1.532193   \n",
       "\n",
       "            p4        g1        g2        g3        g4      stab     stabf  \n",
       "0    -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347  unstable  \n",
       "1    -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957    stable  \n",
       "2    -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471  unstable  \n",
       "3    -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871  unstable  \n",
       "4    -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860  unstable  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995 -1.236256  0.601709  0.779642  0.813512  0.608385  0.023892  unstable  \n",
       "9996 -1.733414  0.502079  0.567242  0.285880  0.366120 -0.025803    stable  \n",
       "9997 -1.975373  0.487838  0.986505  0.149286  0.145984 -0.031810    stable  \n",
       "9998 -0.898510  0.365246  0.587558  0.889118  0.818391  0.037789  unstable  \n",
       "9999 -0.570329  0.073056  0.505441  0.378761  0.942631  0.045263  unstable  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1 Download Electrical Grid Stability Simulated Datadatasets. Classify \"Electrical Grid Stability Simulated Data\" classes using KNN. Use the same data splitting and performance metrics that you have used in previous week (week 7, Q-2). Report your findings including comparison of results with week 7 \n",
    "import pandas as pd\n",
    "local_path = r'E:\\2-学习\\1-Deakin\\24-T1\\SIT720\\Task\\Data_for_UCI_named.csv'\n",
    "df=pd.read_csv(local_path, comment='#')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76a67b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "Accuracy: 0.94\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "F1-score: 0.93\n",
      "Confusion Matrix:\n",
      "[[ 636   88]\n",
      " [  36 1240]]\n",
      "Cross-Validation Scores: [0.7765 0.787  0.7895 0.7855 0.778 ]\n",
      "Mean Cross-Validation Score: 0.7832999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(columns=['stabf'])\n",
    "y = df['stabf']\n",
    "\n",
    "# Split the data into training and testing sets (80% train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()# default as 'Euclidean' distance\n",
    "\n",
    "# Train the KNN model on the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "#knn.fit(X_train,  y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "#y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(knn, X, y, cv=5)  # Perform 5-fold cross-validation\n",
    "\n",
    "# Cross-validation\n",
    "mean_cv_score = cv_scores.mean()\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-Validation Score:\", mean_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc099d",
   "metadata": {},
   "source": [
    "Based on the provided results, we can compare the performance of the KNN (K-Nearest Neighbors) and SVM (Support Vector Machine) with a linear kernel for the task of detecting \"Electrical Grid Stability Simulated Data\".\n",
    "\n",
    "KNN:\n",
    "- Accuracy: 0.94\n",
    "- Precision: 0.94\n",
    "- Recall: 0.93\n",
    "- F1-score: 0.93\n",
    "- Mean Cross-Validation Score: 0.7833\n",
    "\n",
    "SVM with linear kernel:\n",
    "- Accuracy: 0.92\n",
    "- Precision: 0.92\n",
    "- Recall: 0.91\n",
    "- F1-score: 0.91\n",
    "- Mean Cross-Validation Score: 0.92\n",
    "\n",
    "From these results, we can observe the following:\n",
    "\n",
    "1. **Accuracy**: KNN has a slightly higher accuracy of 0.94 compared to 0.92 for SVM with a linear kernel.\n",
    "\n",
    "2. **Precision**: KNN and SVM have the same precision of 0.94 and 0.92, respectively.\n",
    "\n",
    "3. **Recall**: KNN has a slightly higher recall of 0.93 compared to 0.91 for SVM.\n",
    "\n",
    "4. **F1-score**: KNN has a slightly higher F1-score of 0.93 compared to 0.91 for SVM.\n",
    "\n",
    "5. **Confusion Matrix**: Both models have a similar confusion matrix, with KNN having slightly fewer false positives (88 vs. 86) and fewer false negatives (36 vs. 72) compared to SVM.\n",
    "\n",
    "6. **Cross-Validation Score**: SVM with a linear kernel has a higher mean cross-validation score of 0.92 compared to 0.7833 for KNN. This suggests that SVM with a linear kernel might generalize better to unseen data.\n",
    "\n",
    "Overall, both KNN and SVM with a linear kernel performed well on this task, with KNN having slightly better overall performance metrics (accuracy, precision, recall, and F1-score) on the test set. However, SVM with a linear kernel has a higher mean cross-validation score, indicating better generalization to unseen data.\n",
    "\n",
    "It's important to note that the choice between KNN and SVM (or any other algorithm) depends on the specific problem, the characteristics of the data, and the trade-offs between different performance metrics that are most important for the task at hand. Additionally, hyperparameter tuning and feature engineering can significantly impact the performance of both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b83efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for 50-50% Data Splitting Method:\n",
      "Accuracy: 0.97\n",
      "Precision: 0.97\n",
      "Recall: 0.97\n",
      "F1-score: 0.97\n",
      "Confusion Matrix:\n",
      "[[1733   62]\n",
      " [  90 3115]]\n",
      "\n",
      "Performance Metrics for 80-20% Data Splitting Method:\n",
      "Accuracy: 0.92\n",
      "Precision: 0.91\n",
      "Recall: 0.92\n",
      "F1-score: 0.91\n",
      "Confusion Matrix:\n",
      "[[ 639   54]\n",
      " [ 109 1198]]\n"
     ]
    }
   ],
   "source": [
    "# Q2  Load Electrical Grid Stability Simulated Data and create classification model using DT algorithm using 50-50% and 80-20% data splitting methods. Compare performances of these two models and explain impact of difference in data splitting on the performances of the model.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Separate features and target variable\n",
    "X = df.drop('stabf', axis=1)\n",
    "y = df['stabf']\n",
    "\n",
    "# 50-50% data splitting\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# 80-20% data splitting\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Decision Tree Classifier with optimized hyperparameters\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=8, min_samples_split=20, min_samples_leaf=10, max_features='sqrt', random_state=42)\n",
    "\n",
    "# Train the model using 50-50% data splitting method\n",
    "dt_classifier.fit(X_train_50, y_train_50)\n",
    "\n",
    "# Make predictions on the test set using 50-50% data splitting method\n",
    "y_pred_50 = dt_classifier.predict(X_test_50)\n",
    "\n",
    "# Evaluate the model using different metrics for 50-50% data splitting method\n",
    "accuracy_50 = accuracy_score(y_test_50, y_pred_50)\n",
    "precision_50 = precision_score(y_test_50, y_pred_50, average='macro')\n",
    "recall_50 = recall_score(y_test_50, y_pred_50, average='macro')\n",
    "f1_50 = f1_score(y_test_50, y_pred_50, average='macro')\n",
    "conf_matrix_50 = confusion_matrix(y_test_50, y_pred_50)\n",
    "\n",
    "# Train the model using 80-20% data splitting method\n",
    "dt_classifier.fit(X_train_80, y_train_80)\n",
    "\n",
    "# Make predictions on the test set using 80-20% data splitting method\n",
    "y_pred_80 = dt_classifier.predict(X_test_80)\n",
    "\n",
    "# Evaluate the model using different metrics for 80-20% data splitting method\n",
    "accuracy_80 = accuracy_score(y_test_80, y_pred_80)\n",
    "precision_80 = precision_score(y_test_80, y_pred_80, average='macro')\n",
    "recall_80 = recall_score(y_test_80, y_pred_80, average='macro')\n",
    "f1_80 = f1_score(y_test_80, y_pred_80, average='macro')\n",
    "conf_matrix_80 = confusion_matrix(y_test_80, y_pred_80)\n",
    "\n",
    "# Print the performance metrics for both data splitting methods\n",
    "print(\"Performance Metrics for 50-50% Data Splitting Method:\")\n",
    "print(f\"Accuracy: {accuracy_50:.2f}\")\n",
    "print(f\"Precision: {precision_50:.2f}\")\n",
    "print(f\"Recall: {recall_50:.2f}\")\n",
    "print(f\"F1-score: {f1_50:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_50)\n",
    "\n",
    "print(\"\\nPerformance Metrics for 80-20% Data Splitting Method:\")\n",
    "print(f\"Accuracy: {accuracy_80:.2f}\")\n",
    "print(f\"Precision: {precision_80:.2f}\")\n",
    "print(f\"Recall: {recall_80:.2f}\")\n",
    "print(f\"F1-score: {f1_80:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4d157",
   "metadata": {},
   "source": [
    "Based on the provided performance metrics, we can compare the two models and analyze the impact of the difference in data splitting on their performances:\n",
    "\n",
    "1. **Accuracy**:\n",
    "   - 50-50% Data Splitting: Accuracy = 0.97\n",
    "   - 80-20% Data Splitting: Accuracy = 0.92\n",
    "\n",
    "The model trained with 50-50% data splitting has a higher accuracy of 0.97 compared to 0.92 for the model trained with 80-20% data splitting.\n",
    "\n",
    "2. **Precision**:\n",
    "   - 50-50% Data Splitting: Precision = 0.97\n",
    "   - 80-20% Data Splitting: Precision = 0.91\n",
    "\n",
    "The model trained with 50-50% data splitting has a higher precision of 0.97, meaning it has fewer false positives compared to the model trained with 80-20% data splitting, which has a precision of 0.91.\n",
    "\n",
    "3. **Recall**:\n",
    "   - 50-50% Data Splitting: Recall = 0.97\n",
    "   - 80-20% Data Splitting: Recall = 0.92\n",
    "\n",
    "The model trained with 50-50% data splitting has a higher recall of 0.97, indicating that it has fewer false negatives compared to the model trained with 80-20% data splitting, which has a recall of 0.92.\n",
    "\n",
    "4. **F1-score**:\n",
    "   - 50-50% Data Splitting: F1-score = 0.97\n",
    "   - 80-20% Data Splitting: F1-score = 0.91\n",
    "\n",
    "The F1-score, which is the harmonic mean of precision and recall, is higher for the model trained with 50-50% data splitting (0.97) compared to the model trained with 80-20% data splitting (0.91).\n",
    "\n",
    "5. **Confusion Matrix**:\n",
    "   - 50-50% Data Splitting: The confusion matrix shows fewer false positives (62) and fewer false negatives (90) compared to the 80-20% data splitting model.\n",
    "   - 80-20% Data Splitting: The confusion matrix shows more false positives (54) and more false negatives (109) compared to the 50-50% data splitting model.\n",
    "\n",
    "The difference in performance between the two models can be attributed to the difference in data splitting. The 50-50% data splitting method provides a more balanced training and testing set, which can lead to better generalization and higher performance metrics.\n",
    "\n",
    "On the other hand, the 80-20% data splitting method allocates a smaller portion of the data for testing (20%), which can result in a less representative test set and potentially lower performance metrics. Additionally, with a smaller test set, the performance metrics may be more sensitive to outliers or imbalances in the data.\n",
    "\n",
    "It's important to note that the choice of data splitting method depends on the specific problem, the size of the dataset, and the trade-off between model complexity and generalization performance. In general, a larger test set (e.g., 80-20% or even 70-30% splitting) is preferred for a more reliable estimate of the model's performance on unseen data. However, if the dataset is small, a more balanced split like 50-50% or cross-validation techniques may be necessary to make the most of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5df923ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Cityblock Distance Metric:\n",
      "Accuracy: 0.94\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "F1-score: 0.93\n",
      "Confusion Matrix:\n",
      "[[ 633   60]\n",
      " [  58 1249]]\n",
      "\n",
      "KNN with Manhattan Distance Metric:\n",
      "Accuracy: 0.94\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "F1-score: 0.93\n",
      "Confusion Matrix:\n",
      "[[ 633   60]\n",
      " [  58 1249]]\n"
     ]
    }
   ],
   "source": [
    "# Q3 Create two more KNN-based classification models using the dataset used in Q1 by varying distance metrics such as using  cityblock and manhattan. Report the performances of the developed models including Q1 and explain the similarity or differences if any.\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train KNN models with different distance metrics\n",
    "knn_cityblock = KNeighborsClassifier(n_neighbors=5, metric='cityblock')\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "\n",
    "knn_cityblock.fit(X_train_scaled, y_train)\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_cityblock = knn_cityblock.predict(X_test_scaled)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_cityblock = accuracy_score(y_test, y_pred_cityblock)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "precision_cityblock = precision_score(y_test, y_pred_cityblock, average='macro')\n",
    "precision_manhattan = precision_score(y_test, y_pred_manhattan, average='macro')\n",
    "\n",
    "recall_cityblock = recall_score(y_test, y_pred_cityblock, average='macro')\n",
    "recall_manhattan = recall_score(y_test, y_pred_manhattan, average='macro')\n",
    "\n",
    "f1_cityblock = f1_score(y_test, y_pred_cityblock, average='macro')\n",
    "f1_manhattan = f1_score(y_test, y_pred_manhattan, average='macro')\n",
    "\n",
    "conf_matrix_cityblock = confusion_matrix(y_test, y_pred_cityblock)\n",
    "conf_matrix_manhattan = confusion_matrix(y_test, y_pred_manhattan)\n",
    "\n",
    "print(\"KNN with Cityblock Distance Metric:\")\n",
    "print(f\"Accuracy: {accuracy_cityblock:.2f}\")\n",
    "print(f\"Precision: {precision_cityblock:.2f}\")\n",
    "print(f\"Recall: {recall_cityblock:.2f}\")\n",
    "print(f\"F1-score: {f1_cityblock:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_cityblock)\n",
    "\n",
    "print(\"\\nKNN with Manhattan Distance Metric:\")\n",
    "print(f\"Accuracy: {accuracy_manhattan:.2f}\")\n",
    "print(f\"Precision: {precision_manhattan:.2f}\")\n",
    "print(f\"Recall: {recall_manhattan:.2f}\")\n",
    "print(f\"F1-score: {f1_manhattan:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18b040",
   "metadata": {},
   "source": [
    "Based on the provided performance metrics, we can analyze the similarities and differences between the models:\n",
    "\n",
    "KNN with Euclidean Distance Metrics:\n",
    "\n",
    "Accuracy: 0.94\n",
    "Precision: 0.94\n",
    "Recall: 0.93\n",
    "F1-score: 0.93\n",
    "\n",
    "\n",
    "KNN with Cityblock Distance Metric:\n",
    "\n",
    "Accuracy: 0.94 (same as Euclidean)\n",
    "Precision: 0.94 (same as Euclidean)\n",
    "Recall: 0.93 (same as Euclidean)\n",
    "F1-score: 0.93 (same as Euclidean)\n",
    "Confusion Matrix: Similar to Euclidean, but with slightly fewer false positives (60 vs. 88) and more false negatives (58 vs. 36)\n",
    "\n",
    "\n",
    "KNN with Manhattan Distance Metric:\n",
    "\n",
    "Accuracy: 0.94 (same as Euclidean and Cityblock)\n",
    "Precision: 0.94 (same as Euclidean and Cityblock)\n",
    "Recall: 0.93 (same as Euclidean and Cityblock)\n",
    "F1-score: 0.93 (same as Euclidean and Cityblock)\n",
    "Confusion Matrix: Identical to the Cityblock distance metric\n",
    "\n",
    "\n",
    "\n",
    "Similarities:\n",
    "\n",
    "All three models (Euclidean, Cityblock, and Manhattan) have the same overall accuracy, precision, recall, and F1-score of 0.94, 0.94, 0.93, and 0.93, respectively.\n",
    "The confusion matrices are similar, with a slight difference in the number of false positives and false negatives between Q1 and the Cityblock/Manhattan distance metrics.\n",
    "\n",
    "Differences:\n",
    "\n",
    "The only notable difference is in the confusion matrices, where the Cityblock and Manhattan distance metrics have slightly fewer false positives (60 vs. 88 in Q1) and more false negatives (58 vs. 36 in Euclidean) compared to the baseline Euclidean model.\n",
    "\n",
    "Based on these results, it seems that changing the distance metric from the default Euclidean to Cityblock or Manhattan did not significantly impact the overall performance of the KNN models. All three models performed similarly well, with only minor differences in the distribution of false positives and false negatives.\n",
    "It's worth noting that the choice of distance metric in KNN can impact the model's performance, especially when dealing with different types of data distributions or feature scales. However, in this particular case, the differences are relatively minor, and all three models achieved similar and reasonably good performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
