{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25816a9a",
   "metadata": {},
   "source": [
    "### Task 10.1\n",
    "* Name:LI WAN\n",
    "* Student Number:223718804\n",
    "* E-mail:s223718804@deakin.edu.au\n",
    "* Course:SIT720\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7524a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43871</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43872</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>224</td>\n",
       "      <td>82</td>\n",
       "      <td>159</td>\n",
       "      <td>224</td>\n",
       "      <td>82</td>\n",
       "      <td>159</td>\n",
       "      <td>224</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43873</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43874</th>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43875</th>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43876 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  t_8  t_9  ...  t_91  t_92  \\\n",
       "0      112  274  158  215  274  158  215  298   76  208  ...    71   297   \n",
       "1       82  208  187  208  172  117  172  117  172  117  ...    81   240   \n",
       "2       16  110  240  117  240  117  240  117  240  117  ...    65   112   \n",
       "3       82  208  187  208  172  117  172  117  172  117  ...   208   302   \n",
       "4       82  240  117  240  117  240  117  240  117  172  ...   209   260   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "43871   82  240  117  240  117  240  117  240  117  172  ...   141   260   \n",
       "43872   82  240  117  240  117  240  117  240  117  172  ...   159   224   \n",
       "43873   82  240  117  240  117  240  117  240  117  172  ...   260   141   \n",
       "43874   82  240  117  240  117  240  117  240  117  172  ...   141   260   \n",
       "43875  112  274  158  215  274  158  215  298   76  208  ...    71   297   \n",
       "\n",
       "       t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0       135   171   215    35   208    56    71        1  \n",
       "1       117    71   297   135   171   215    35        1  \n",
       "2       123    65   112   123    65   113   112        1  \n",
       "3       208   302   187   208   302   228   302        1  \n",
       "4        40   209   260   141   260   141   260        1  \n",
       "...     ...   ...   ...   ...   ...   ...   ...      ...  \n",
       "43871   141   260   141   260   141   260   141        1  \n",
       "43872    82   159   224    82   159   224    82        1  \n",
       "43873   260   141   260   141   260   141   260        1  \n",
       "43874   141   260   141   260   141   260   141        1  \n",
       "43875   135   171   215    35   208    56    71        1  \n",
       "\n",
       "[43876 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1 Create a MLP model with 10 hidden layers using  \"data.csv\" dataset and report performances using appropriate metrics. \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "local_path = r'E:\\2-å­¦ä¹ \\1-Deakin\\24-T1\\SIT720\\Task\\data_10.csv'\n",
    "df=pd.read_csv(local_path, comment='#')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5dd69",
   "metadata": {},
   "source": [
    "The accuracy score reported is 0.9826, which means that the model correctly classified 98.26% of the test instances.\n",
    "However, the classification report provides more insights into the model's performance. It shows that the model has a high precision of 0.98 for the positive class (malware = 1), which means that when the model predicts an instance as malware, it is correct 98% of the time. However, the recall for the negative class (malware = 0) is very low at 0.33, which means that the model misses many instances of non-malware and incorrectly classifies them as malware.\n",
    "The F1-score, which is the harmonic mean of precision and recall, is low for the negative class (0.50) and high for the positive class (0.99). This imbalance is likely due to the class imbalance in the dataset, where there are significantly more instances of malware (8550) compared to non-malware (226).\n",
    "Overall, while the model has a high accuracy, it performs poorly in correctly identifying non-malware instances. This issue could potentially be addressed by using techniques such as oversampling the minority class or adjusting the classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33196b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9826800364630811\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.33      0.50       226\n",
      "           1       0.98      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.98      8776\n",
      "   macro avg       0.98      0.67      0.74      8776\n",
      "weighted avg       0.98      0.98      0.98      8776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into features and target variable\n",
    "X = df.drop(columns=['malware'])  # Assuming 'malware' is the name of the target variable\n",
    "y = df['malware']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create MLP model with 10 hidden layers\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Report performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469e0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Activation Function: identity, Accuracy: 0.9814266180492252\n",
      "Activation Function: logistic, Accuracy: 0.9742479489516864\n",
      "Activation Function: tanh, Accuracy: 0.9742479489516864\n",
      "Activation Function: relu, Accuracy: 0.9826800364630811\n"
     ]
    }
   ],
   "source": [
    "# Q2 Analyse impact of different activation function with adam solver on the model.\n",
    "# Define a list of activation functions to test\n",
    "activation_functions = ['identity', 'logistic', 'tanh', 'relu']\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Loop through each activation function\n",
    "for activation in activation_functions:\n",
    "    # Create MLP model with Adam solver and current activation function\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=activation, solver='adam', max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the test set\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[activation] = accuracy\n",
    "\n",
    "# Print results\n",
    "print(\"Results:\")\n",
    "for activation, accuracy in results.items():\n",
    "    print(f\"Activation Function: {activation}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ea358",
   "metadata": {},
   "source": [
    "The results show that the activation function with the highest accuracy on the test set is the 'relu' (Rectified Linear Unit) activation function, achieving an accuracy of 0.9827 (98.27%).\n",
    "The 'identity' activation function comes second with an accuracy of 0.9814 (98.14%), followed by 'logistic' and 'tanh' activation functions, both achieving an accuracy of 0.9742 (97.42%).\n",
    "These results suggest that the 'relu' activation function performs the best for this particular dataset and model architecture. The 'identity' activation function also performs reasonably well, while the 'logistic' and 'tanh' activation functions perform slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3f779",
   "metadata": {},
   "source": [
    "Q3: Explain your findings and report the best performance.\n",
    "Based on the results from the two Questions provided, we can make the following observations and report the best performance:\n",
    "\n",
    "1. **Dataset Characteristics**:\n",
    "   - The dataset appears to be imbalanced, with a significantly higher number of instances belonging to the positive class (malware = 1) compared to the negative class (malware = 0). Specifically, there are 8550 instances of malware and only 226 instances of non-malware.\n",
    "\n",
    "2. **Model Performance on the Original Setup**:\n",
    "   - The Multi-Layer Perceptron (MLP) classifier with 10 hidden layers and the default settings achieved an overall accuracy of 0.9826 (98.26%) on the test set.\n",
    "   - However, the classification report revealed that the model performed poorly in correctly identifying non-malware instances, with a recall of only 0.33 for the negative class.\n",
    "   - This issue is likely due to the class imbalance in the dataset, where the model is biased towards the majority class (malware).\n",
    "\n",
    "3. **Impact of Activation Functions**:\n",
    "   - The second question evaluated the impact of different activation functions (identity, logistic, tanh, and relu) on the model's performance.\n",
    "   - The 'relu' (Rectified Linear Unit) activation function achieved the highest accuracy of 0.9827 (98.27%) on the test set.\n",
    "   - The 'identity' activation function also performed well, with an accuracy of 0.9814 (98.14%).\n",
    "   - The 'logistic' and 'tanh' activation functions had slightly lower accuracies of 0.9742 (97.42%).  \n",
    "   \n",
    "4. **Best Performance**:\n",
    "   - Based on the results, the best performance was achieved by the MLP classifier with 10 hidden layers, using the 'relu' activation function and the Adam solver.\n",
    "   - This configuration yielded an overall accuracy of 0.9827 (98.27%) on the test set.\n",
    "\n",
    "However, it's important to note that while the model achieved high overall accuracy, its performance in correctly identifying non-malware instances was poor due to the class imbalance. To improve the model's performance on the minority class, additional techniques such as oversampling, undersampling, or adjusting the classification threshold may be necessary.\n",
    "\n",
    "Additionally, it's recommended to evaluate the model's performance using metrics that are more suitable for imbalanced datasets, such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC), rather than relying solely on accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
